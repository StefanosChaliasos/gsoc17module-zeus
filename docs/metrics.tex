\documentclass{article}

\usepackage{url}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{algpseudocode}

\presetkeys{todonotes}{fancyline, color=yellow!30}{}

\date{}
\begin{document}

\title{Metrics for the AsiaCrypt16 Implementation} 

\author{Vitalis Salis}

\maketitle
\begin{abstract}
  We have computed some metrics for the prototype implementation
  \cite{prototype} of a mixnet based on the shuffle argument proposed
  by Fauzi et al \cite{shufflearg}. The goal of these metrics is to
  identify aspects of the code that are slow and find suitable
  replacements for them.
\end{abstract}

\section{Introduction}

The prototype implementation of the mixnet proposed by Fauzi et al,
produces multiple implementation difficulties. On implementations of
cryptographic protocols it is typical to use C for your cryptographic
computation. Yet the prototype is implemented using python, so it has
to switch between python and C for its computations. This may be a
bottleneck of the prototype and the reason some operations are slower
than they should. Another reason may be that the underlying C
cryptographic operations themselves are not efficient, and a different
C implementation might improve matters. The two reasons are not
exclusive, and one might compound the other. We start by investigating
the first. 

\section{Metrics}

Table 1 contains a list of metrics for the various operations of the
prototype. Most of the time is taken by the prover and the verifier,
as expected, because these have the most computations that produce a
context switch between Python and C.

\begin{table}
\begin{tabular}{ |p{3cm}|p{5cm}|p{3cm}|  }
    \hline
    \multicolumn{3}{|c|}{Metrics}\\
    \hline
    Operation & Short Description & Time per 100 voters\\
    \hline
    Initialization & Creates the elliptic Curve and private keys & 364ms\\
    Encryption & Encrypts the votes & 674ms\\
    Random Permutations & Creates random numbers & 1ms\\
    Proof & The shuffle & 2085ms\\
    Verification & Verification of the shuffle & 2738ms\\
    Decryption & Decrypts the votes & 489ms\\
    \hline
\end{tabular}
\caption{Metrics}
\end{table}

The time taken by each of these operations is linear, meaning that for
200 ciphertexts you just double the numbers on the table.

\section{Context Switches}

A context switch happens when a python program communicates with a C
program for various cryptographic computations. The reasoning behind
believing that a context switch may be the bottleneck of the application
is that python needs to create a PyObject containing the
data it wants to communicate, and C also needs to create a PyObject to
return the result of the computations.

The prover has various steps. In order to validate our theory about context
switches we measured each of these steps. Two of those steps, while having
the same numberof iterations, had a significant time difference
(100ms compared to 700ms).

First we attributed the time difference to various calls to zip and to tuple
creation. After removing all the calls to zip we didn't notice any significant
difference. This seemed to validate the context switches theory, because the
slower step contained more context switches per iteration.

But that's not the case. Using cProfile we identified the main
reason behind this difference. The slower step does more multiplications on
elliptic curve elements. While it is expected that multiplication will be slower
than addition, the difference was enough to dismiss the context switches theory.

Multiplication on our elliptic curve elements takes 575ms per 300
multiplications, while addition takes 5ms for 400 additions. If the real
problem were context switches, then the addition wouldn't have such a huge
difference with the multiplication, because it has more operations hence
more context switches.


\section{Solutions}

Since the real bottleneck are the context switches, the most obvious
solutions is to limit them. 
\todo[inline]{PL: In fact this is still to be shown; we do not really
  know yet that the context switches take most of the time. We know
  that the C crypto operations take most of the time. But do they take
  more time than the same operations executed as part of a
  \emph{single} context switch?}

The prover and the verifier that take the most time, also have the
most loops that do a context switch for each voter. We can avoid that
by using universal functions provided by numpy\cite{numpy} to do
vectorized operations.


\bibliographystyle{plain}
\bibliography{metrics}

\end{document} 
